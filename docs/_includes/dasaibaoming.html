<div class="container">
  <div class="user-details">
    <h3>比赛报名</h3>
    <h5>参赛资格</h5>
    <p align="left">中美青年创客大赛对任何中国公民或美国公民、或在中国或美国获得永久合法居留权的个人开放。报名者年龄应在大赛报名起始日时符合）的员工，或上述任何实体的母公司或子公司的员工；(2)上述任何实体的任何一名员工的直系亲属。</p>
    <!--<div align="left"> 
      <ul>
        <li>The sequential version is run on machines containing 8 core 3.2 GHz Intel Core i7 processors.</li>
        <li>The Cuda version is run on machines containing <a class="link" target="_blank" href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080/">NVIDIA GeForce GTX 1080</a> . </li>
      </ul>
    </div>  -->
    
    <h5>参赛团队要求</h5>
    <div align="left"> 
      <ul>
        <li>团队形式报名时，团队总人数不得超过5人（含领队），领队为团队的联系人和代表。</li>
        <li>直至比赛正式开始前，领队可替换一位或多位成员，领队不可更换。</li>
        <li>参赛者可参加到多个团队中，但至多作为其中一支团队的领队。</li>
      </ul>
    </div>  
    
    <h5>报名流程</h5>
    
    <p align="left">We first construct a sequential version based on the Friedman’s 1999 paper of gradient boosting tree, 
      since our dataset is a binary value attribute dataset and our model is expected to be a binary classifier. 
      This particular model has been discussed in Friedman’s paper. In its paper, Friedman proposed one-step Newton-Raphson 
      approximation of the linear optimization of the loss function. Pseudo-code is provided in <i>Figure5</i>. We directly implemented that approximation since it would 
      take less time than computing the whole gradient of all possible outcome and selecting the best one from it. </p>

    <p align="left">The construction algorithm of decision tree is based on pseudo-code provided by 10-701 course, 
      and we are using entropy approach (ID3 algorithm) to select the best attribute from whole dataset in each iteration. </p>
    
    <div><img src="{{ "/assets/img/gradientboosting.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 5: Gradient boosting with approximation pseudocode</p></div>
 
    <h5>Parallel Attempt 1</h5>
    <p align="left"> In the first attempt of parallelization with CUDA, we simply translate the algorithm directly into 
      CUDA version. We have carefully analyzed the program and we discovered that our approach should be different from 
      previous <a class="link" href="http://zhanpengfang.github.io/418home.html" target="_blank">year project</a> approach. They are using thread parallelization during construction of a decision tree. 
      Such a top-down approach would provide great parallelization on CPU threads with OpenMP. However, since gradient 
      boosting is construct based on weak decision trees that has lower levels, the same top-down approach would waste 
      most of the computing power provided by CUDA and GTX1080. Thus we need to figure out a new data parallel approach. 
      We decided to store Data in a huge GPU array of 130 attributes for each line. Each CUDA thread should read data 
      through bias or indexing of that array. Except the second step in GDBT main loop body, all other three steps are 
      loosely connected. They are suitable to be directly compute through function mapping. The sum of values are done 
      with prefix-sum like reduction kernel. For building decision tree, we use extra two data length int array to represent 
      as mask of values for each decision node. This would avoid the necessity of copying data and distribute new copy 
      as we have done in the sequential version.</p>

     <p align="left"> However, during the implementation, we have realized this approach does not provide good parallelization. 
      The data communication overhead between GBDT steps area high, and each step is mapping their functions to data 
      for only one purpose, which leads to low compute ratio. Furthermore, the way we storing the data consumes lots of 
      memory, but lots of them are only access as an indicator of having or not having an attribute. Therefore, we made 
      several improvements and have a different version of parallelization.</p>

    <h5>Parallel Attempt 2</h5>
    <p align="left">In second version of CUDA parallelization, we have done several things to improve the parallelization 
      of our model. The first thing we do is de-code the algorithm into tiny compute steps to determine potential 
      parallelization in each steps. Thus we figured out with the approximate approach, we are able to compute GBDT 
      loop body step 3, 4 and step 1 of next loop with one reduction and one mapper function. We also figured out during 
      the construction of the tree, several components could be calculating at the same time, since they do not have 
      dependency of each other. Thus we used a single reduction to compute four different attributes to burst the 
      construction of decision tree. </p>
    
    <p align="left">We also changed our data representation, for we have the binary attribute dataset, we could infer 
      the value of attributes by only storing the data that have positive attributes. Thus the data set has been reduced 
      to less than half of the original dataspace. We also stored all intermediate result in the extra few slots of dataline,
      which reduce the communication overhead. Each step only need to access those intermediate result they needed from 
      the mutually agreed position of each dataline. </p>
    
    <p align="left">However, even the improvement significantly speed up our code, we are also introduced new bottleneck 
      into our code. In this version, each thread need to sequentially loop through the entire attribute list to figure out 
      whether a particular attribute is positive or not. Thus we modified our program the third time to deal with newly 
      introduced bottleneck.</p>
    
    <h5>Parallel Attempt 3</h5>
    <p align="left">We observed that in the dataset, all attributes are stored in a ascending order. Thus we would be 
      able to use Binary search to improve the speed up of finding attributes from the current dataline. </p>
    
    <p align="left">After we did this improvement, we analyzed our code again, but we haven’t be able to figure out 
      other point for improving parallelization under current algorithm.</p>
  
    <h3>Results</h3>
    <p align="left">The final result met our expectation. Figure 6 shows how our cuda version code compare to the 
      sequential version code. With increasing of data, cuda shows remarkable parallelization ability and it could 
      achieve about 160x speed up with 1 million dataset. </p>
    
    <p align="left">We have further breakdown the the time of our current cuda model in 1 million dataset to determine 
      the bottleneck of our code. As shown in Figure 7 , no matter how many data is request for copy from host memory 
      to device memory, the CUDA memory copy time of the data does not change much. Most of the time is consumed by 
      building the model, since we have already parallelized the current algorithm, to improve the model building time, 
      it could be only done by adopting faster algorithms.</p>
    
    <p align="left">Distributed GBDT library xgboost does have a CUDA plugin available. However we cannot use it since it 
      cannot compile without cmake. Based on their introductory material, we only know it has ability to execute 1 million 
      records of 50 column in 500 iterations in 193 seconds. Our dataset does not have too much residuals, and it would left 
      with extreme small residuals, which would be considered as zero after 25 iterations. Thus with our approach, and the 
      dataset, we would only need 22 seconds to execute. </p>

    <div><img src="{{ "/assets/img/chart.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 6: Performance analysis and comparison</p></div>
    <div><img src="{{ "/assets/img/breakdown.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 7: Parallel implementation time breakdown</p></div>
    
    <h3>Further Research</h3>
    <p align="left">In future research, it is possible to use other algorithms that consumes less time than our 
      current algorithm which is based on Friedman 1999 paper. The paper came up with gradient boosting technique 
      almost 20 years ago, more parallelizable approach would emerge in the passing years, thus GBDT could take less 
      time than our approach. Such as the histogram GBDT algorithm could bring down the execution time even further. </p>
    
    <p align="left">Also, our current implementation is working on single GPU, with expansion of data, the size of  
      dataset will exceed total GPU device memory. Therefore, in order to support larger datasets, CuDB could be extended 
      to support either multiple GPU with a shared device memory or streamly computing of decision tree in single GPU.</p>
    
    <h3>References</h3>
    <p align="left"><cite>Friedman, J. H. (1999). Greedy function approximation: A gradient boosting machine. Retrieved May 12, 2017, 
      from <a class="link" target="_blank" href="http://projecteuclid.org/euclid.aos/1013203451">http://projecteuclid.org/euclid.aos/1013203451</a>.</cite></p>
    <p align="left"><cite>Xing, E. (2015). Decision Tree. <a class="link" target="_blank" href="http://www.cs.cmu.edu/~epxing/Class/10701/slides/DT15New.pdf">Lecture</a>.</cite></p>  
    <p align="left"><cite>Scalable and Flexible Gradient Boosting. (n.d.). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest</a>.</cite></p> 
    <p align="left"><cite>Gradient boosting. (2017, April 30). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Gradient_boosting">https://en.wikipedia.org/wiki/Gradient_boosting</a>.</cite></p> 
  </div>
</div>
