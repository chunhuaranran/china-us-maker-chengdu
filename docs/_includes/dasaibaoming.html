<div class="container">
  <div class="user-details">
    <h3>比赛报名</h3>
    
     <div class="row user-projects" align="center">
    <a target="_blank" class="project-link" href="http://www.chinaus-maker.org/chinaus-maker/bmcs/index.html">点击报名</a>
    </div>
    
    <h5>参赛资格</h5>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 中美青年创客大赛对任何中国公民或美国公民、或在中国或美国获得永久合法居留权的个人开放。报名者年龄应在比赛报名起始日时符合18周岁以上或40周岁以下的要求（2018年比赛要求报名者出生日期不早于1978年4月1日并且不晚于2000年4月1日）。参赛者有责任了解其出席并参加此次活动的合法权利，并须携带政府颁发的官方有效身份证明参加比赛。</p>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 参赛者不能为(1)承办单位（即，中国（教育部）留学服务中心、清华大学、北京歌华文化发展集团和谷歌信息技术（中国）有限公司）的员工，或上述任何实体的母公司或子公司的员工；(2)上述任何实体的任何一名员工的直系亲属。</p>
    <!--<div align="left"> 
      <ul>
        <li>The sequential version is run on machines containing 8 core 3.2 GHz Intel Core i7 processors.</li>
        <li>The Cuda version is run on machines containing <a class="link" target="_blank" href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080/">NVIDIA GeForce GTX 1080</a> . </li>
      </ul>
    </div>  -->
    
    <h5>团队报名</h5>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 报名者需通过登录<a class="link" target="_blank" href="https://www.chinaus-maker.org/">中美青年创客大赛官方网站</a>完成团队报名。报名者可采个人或团队方式参赛，所有个人及团队报名时需要选择参赛分赛区，并根据所选分赛区要求统一参赛，同场竞技。大赛将不会为个人或团队单独设立比赛。</p>
        
    <h5>比赛地点</h5>
        <p align="center">地址：西南交通大学（犀浦校区）大学生创新创业教育中心（西二门附近三食堂三楼）</p>
        <p align="center">公共交通：<br/>
                          公交：119路坐到西南交大新校区西二门 <br/>
                          地铁：地铁2号线到犀浦站（终点站）下，然后沿天府路步行约1km，由西南交通大学西二门进入。</p>                                          
    
    <h5>参赛团队要求</h5>
        <div align="left"> 
        <ul>
        <li>团队形式报名时，团队总人数不得超过5人（含领队），领队为团队的联系人和代表。</li>
        <li>直至比赛正式开始前，领队可替换一位或多位成员，领队不可更换。</li>
        <li>参赛者可参加到多个团队中，但至多作为其中一支团队的领队。</li>
        <li>参赛个人或团队需在报名时签署中美青年创客大赛参赛者声明。</li>
        </ul>
        </div>   
    
    <h5> 参赛作品要求 </h5>
        <div align="left">  
        <ul> 
        <li>竞赛主题要求</li>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 大赛倡导参赛者以促进社会可持续发展作为竞赛主题，关注社区、教育、环保、健康、能源、交通等领域，产生的创意需契合比赛主题，并通过结合创新理念和前沿科技，打造具有社会意义和产业价值的作品。</p>
        <li>竞赛创新性要求</li>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 团队可在比赛过程中对作品进行持续的改进，提交的解决方案须具有想象力和创新性；大赛组委会和各分赛区选拔赛承办单位将对其产品进行创新性检索，并将检索结果提交评审委员会作为评分参考。</p>
        <li>作品呈现要求</li>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 作品原型要求基于开源软、硬件平台完成；参赛者需要在比赛现场完成设计并制作出可演示的产品原型；作品原型应呈现为实现一定创新功能的智能硬件或软件。</p>
        <li>作品原型要求基于开源软、硬件平台完成</li>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 参赛者需要在分赛区预选赛阶段现场完成设计工作，并制作出可演示的产品原型。从分赛区预选赛晋级决赛的团队，需要在决赛阶段现场完成针对产品原型的改进、升级和测试等工作。作品原型应呈现为实现一定创新功能的硬件或软件。</p>
        <li>技术平台要求</li>
        <p align="left"> &nbsp &nbsp &nbsp &nbsp 组委会将提供大赛可采用的竞赛技术平台的参考方案，参赛者也可自行选择技术平台和使用相应的工具和设备。</p>
       </ul>
       </div> 
   
     <div class="row user-projects">
     <div class="twelve columns images">
     <div class="four columns images">
       <img alt="other" src="{{ "/assets/img/1.jpg" | prepend: site.baseurl }}" width="98%" height="98%"/>
     </div>
     <div class="four columns images">
       <img alt="other" src="{{ "/assets/img/2.jpg" | prepend: site.baseurl }}" width="98%" height="98%"/>
     </div>
     <div class="four columns" align="center">
       <img alt="other" src="{{ "/assets/img/3.jpg" | prepend: site.baseurl }}" width="98%" height="98%"/>
     </div> 
     </div>
     </div>
   
    <div class="row user-projects">
     <div class="six columns images">
       <img alt="other" src="{{ "/assets/img/4.jpg" | prepend: site.baseurl }}" width="98%" height="98%"/>
     </div>
     <div class="six columns" align="center">
       <img alt="other" src="{{ "/assets/img/5.jpg" | prepend: site.baseurl }}" width="98%" height="98%"/>
     </div> 
     </div>
    
    <h5> 知识产权要求 </h5>
        <div align="left">  
        <ul> 
        <li>参赛者必须保证作品的原创性，不得侵犯任何第三方的知识产权或其他权利，且内容符合可适用的法律、法规（包括但不限于中华人民共和国、美利坚合众国的相关法律、法规）。参赛者同意对因侵犯第三方知识产权或其他权利而导致的请求和索赔负全部责任，并保护比赛的主办方、承办方及其代理人并为其辩解，使其不受任何损失赔偿的请求或追诉。</li>
        <li>参赛作品的知识产权归参赛者所有，但应适当兼顾到竞赛主办和承办单位的权益。中华人民共和国教育部作为大赛主办单位，中国（教育部）  留学服务中心、清华大学、北京歌华文化发展集团和谷歌信息技术（中国）有限公司作为大赛承办单位，拥有在全世界范围内永久免费使用本届参赛作品进行演示、部分或全部出版的权利（不涉及技术细节），大赛承办单位的其他全资子公司也拥有上述权利。如果大赛承办单位以其它目的使用参赛作品，需与参赛团队协商，经参赛团队同意后，签署有关对参赛作品使用的协议。</li>
        <li>在可适用的法律允许范围内，大赛工作组保留本规则的最终解释权。</li>
        </ul>
        </div>
    
    <!--<p align="left">We first construct a sequential version based on the Friedman’s 1999 paper of gradient boosting tree, 
      since our dataset is a binary value attribute dataset and our model is expected to be a binary classifier. 
      This particular model has been discussed in Friedman’s paper. In its paper, Friedman proposed one-step Newton-Raphson 
      approximation of the linear optimization of the loss function. Pseudo-code is provided in <i>Figure5</i>. We directly implemented that approximation since it would 
      take less time than computing the whole gradient of all possible outcome and selecting the best one from it. </p>

    <p align="left">The construction algorithm of decision tree is based on pseudo-code provided by 10-701 course, 
      and we are using entropy approach (ID3 algorithm) to select the best attribute from whole dataset in each iteration. </p>
    
    <div><img src="{{ "/assets/img/gradientboosting.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 5: Gradient boosting with approximation pseudocode</p></div>
 
    <h5>Parallel Attempt 1</h5>
    <p align="left"> In the first attempt of parallelization with CUDA, we simply translate the algorithm directly into 
      CUDA version. We have carefully analyzed the program and we discovered that our approach should be different from 
      previous <a class="link" href="http://zhanpengfang.github.io/418home.html" target="_blank">year project</a> approach. They are using thread parallelization during construction of a decision tree. 
      Such a top-down approach would provide great parallelization on CPU threads with OpenMP. However, since gradient 
      boosting is construct based on weak decision trees that has lower levels, the same top-down approach would waste 
      most of the computing power provided by CUDA and GTX1080. Thus we need to figure out a new data parallel approach. 
      We decided to store Data in a huge GPU array of 130 attributes for each line. Each CUDA thread should read data 
      through bias or indexing of that array. Except the second step in GDBT main loop body, all other three steps are 
      loosely connected. They are suitable to be directly compute through function mapping. The sum of values are done 
      with prefix-sum like reduction kernel. For building decision tree, we use extra two data length int array to represent 
      as mask of values for each decision node. This would avoid the necessity of copying data and distribute new copy 
      as we have done in the sequential version.</p>

     <p align="left"> However, during the implementation, we have realized this approach does not provide good parallelization. 
      The data communication overhead between GBDT steps area high, and each step is mapping their functions to data 
      for only one purpose, which leads to low compute ratio. Furthermore, the way we storing the data consumes lots of 
      memory, but lots of them are only access as an indicator of having or not having an attribute. Therefore, we made 
      several improvements and have a different version of parallelization.</p>

    <h5>Parallel Attempt 2</h5>
    <p align="left">In second version of CUDA parallelization, we have done several things to improve the parallelization 
      of our model. The first thing we do is de-code the algorithm into tiny compute steps to determine potential 
      parallelization in each steps. Thus we figured out with the approximate approach, we are able to compute GBDT 
      loop body step 3, 4 and step 1 of next loop with one reduction and one mapper function. We also figured out during 
      the construction of the tree, several components could be calculating at the same time, since they do not have 
      dependency of each other. Thus we used a single reduction to compute four different attributes to burst the 
      construction of decision tree. </p>
    
    <p align="left">We also changed our data representation, for we have the binary attribute dataset, we could infer 
      the value of attributes by only storing the data that have positive attributes. Thus the data set has been reduced 
      to less than half of the original dataspace. We also stored all intermediate result in the extra few slots of dataline,
      which reduce the communication overhead. Each step only need to access those intermediate result they needed from 
      the mutually agreed position of each dataline. </p>
    
    <p align="left">However, even the improvement significantly speed up our code, we are also introduced new bottleneck 
      into our code. In this version, each thread need to sequentially loop through the entire attribute list to figure out 
      whether a particular attribute is positive or not. Thus we modified our program the third time to deal with newly 
      introduced bottleneck.</p>
    
    <h5>Parallel Attempt 3</h5>
    <p align="left">We observed that in the dataset, all attributes are stored in a ascending order. Thus we would be 
      able to use Binary search to improve the speed up of finding attributes from the current dataline. </p>
    
    <p align="left">After we did this improvement, we analyzed our code again, but we haven’t be able to figure out 
      other point for improving parallelization under current algorithm.</p>
  
    <h3>Results</h3>
    <p align="left">The final result met our expectation. Figure 6 shows how our cuda version code compare to the 
      sequential version code. With increasing of data, cuda shows remarkable parallelization ability and it could 
      achieve about 160x speed up with 1 million dataset. </p>
    
    <p align="left">We have further breakdown the the time of our current cuda model in 1 million dataset to determine 
      the bottleneck of our code. As shown in Figure 7 , no matter how many data is request for copy from host memory 
      to device memory, the CUDA memory copy time of the data does not change much. Most of the time is consumed by 
      building the model, since we have already parallelized the current algorithm, to improve the model building time, 
      it could be only done by adopting faster algorithms.</p>
    
    <p align="left">Distributed GBDT library xgboost does have a CUDA plugin available. However we cannot use it since it 
      cannot compile without cmake. Based on their introductory material, we only know it has ability to execute 1 million 
      records of 50 column in 500 iterations in 193 seconds. Our dataset does not have too much residuals, and it would left 
      with extreme small residuals, which would be considered as zero after 25 iterations. Thus with our approach, and the 
      dataset, we would only need 22 seconds to execute. </p>

    <div><img src="{{ "/assets/img/chart.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 6: Performance analysis and comparison</p></div>
    <div><img src="{{ "/assets/img/breakdown.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 7: Parallel implementation time breakdown</p></div>
    
    <h3>Further Research</h3>
    <p align="left">In future research, it is possible to use other algorithms that consumes less time than our 
      current algorithm which is based on Friedman 1999 paper. The paper came up with gradient boosting technique 
      almost 20 years ago, more parallelizable approach would emerge in the passing years, thus GBDT could take less 
      time than our approach. Such as the histogram GBDT algorithm could bring down the execution time even further. </p>
    
    <p align="left">Also, our current implementation is working on single GPU, with expansion of data, the size of  
      dataset will exceed total GPU device memory. Therefore, in order to support larger datasets, CuDB could be extended 
      to support either multiple GPU with a shared device memory or streamly computing of decision tree in single GPU.</p>
    
    <h3>References</h3>
    <p align="left"><cite>Friedman, J. H. (1999). Greedy function approximation: A gradient boosting machine. Retrieved May 12, 2017, 
      from <a class="link" target="_blank" href="http://projecteuclid.org/euclid.aos/1013203451">http://projecteuclid.org/euclid.aos/1013203451</a>.</cite></p>
    <p align="left"><cite>Xing, E. (2015). Decision Tree. <a class="link" target="_blank" href="http://www.cs.cmu.edu/~epxing/Class/10701/slides/DT15New.pdf">Lecture</a>.</cite></p>  
    <p align="left"><cite>Scalable and Flexible Gradient Boosting. (n.d.). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest</a>.</cite></p> 
    <p align="left"><cite>Gradient boosting. (2017, April 30). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Gradient_boosting">https://en.wikipedia.org/wiki/Gradient_boosting</a>.</cite></p> -->
  </div>
</div>
