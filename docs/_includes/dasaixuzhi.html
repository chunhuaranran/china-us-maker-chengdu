<div class="container">
  <div class="user-details">
    <h3> 大赛须知 </h3>
    <h5> 大赛主办方 </h5>
    <p align="left"> 2018年中美青年创客大赛由中华人民共和国教育部主办，中国（教育部）留学服务中心、清华大学、北京歌华文化发展集团、谷歌信息技术（中国）有限公司共同承办。分赛区选拔赛由各地承办机构组织。</p>
    <!--<div><img src="{{ "/assets/img/img_1.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 1: Decision tree example</p></div>-->
    
    <h5> 大赛流程 </h5>
    <div align="left">  
      <ul align="center"> 
        <p>第一阶段：大赛启动、参赛选手报名和分赛区选拔赛。</p>
        <p>第二阶段：决赛入围团队优化作品。</p>
        <p>第三阶段：2018中美青年创客大赛决赛。</p>
      </ul>
    </div> 
  </div>   
    <!--<p align="left">Different algorithms can be used to learn a decision tree from training data set. CuDB uses ID3 learning to construct decision trees.
      The pseudocode is shown below in <i>Figure 2</i>. In each iteration, it traverses the set of unvisited attributes and choose select the best 
      attribute that provides the most information. This is achieved by calculating the 
      <a class="link" target="_blank" href="http://www.cs.cmu.edu/~roni/10601-slides/ch3.pdf">entropy</a> 
      and choose the attribute with the largest information gain. Then it splits the data according to the selected attribute, 
      and recurse on each of the subset. </p>-->
    
    <!--<div><img src="{{ "/assets/img/id3.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 2: ID3 pseudocode</p></div>-->
   <div class="user-details">
   <h5> 参赛作品要求 </h5>
   </div>
   <div class="row user-projects">
    <div class="six columns">
      <img alt="poster" src="{{ "/assets/img/poster.jpg" | prepend: site.baseurl }}" />
    </div>
    <div class="six columns" align="center">
      
      <ul> 
        <p align="left">竞赛主题要求</p>
        <p align="left"> 大赛倡导参赛者以促进社会可持续发展作为竞赛主题，关注社区、教育、环保、健康、能源、交通等领域，产生的创意需契合比赛主题，并通过结合创新理念和前沿科技，打造具有社会意义和产业价值的作品。</p>
        <p align="left">竞赛创新性要求</p>
        <p align="left"> 团队可在比赛过程中对作品进行持续的改进，提交的解决方案须具有想象力和创新性；大赛组委会和各分赛区选拔赛承办单位将对其产品进行创新性检索，并将检索结果提交评审委员会作为评分参考。</p>
        <li align="left">作品呈现要求</li>
        <p align="left"> 作品原型要求基于开源软、硬件平台完成；参赛者需要在比赛现场完成设计并制作出可演示的产品原型；作品原型应呈现为实现一定创新功能的智能硬件或软件。</p>
        <li align="left">作品原型要求基于开源软、硬件平台完成</li>
        <p align="left"> 参赛者需要在分赛区预选赛阶段现场完成设计工作，并制作出可演示的产品原型。从分赛区预选赛晋级决赛的团队，需要在决赛阶段现场完成针对产品原型的改进、升级和测试等工作。作品原型应呈现为实现一定创新功能的硬件或软件。</p>
        <li align="left">技术平台要求</li>
        <p align="left"> 组委会将提供大赛可采用的竞赛技术平台的参考方案，参赛者也可自行选择技术平台和使用相应的工具和设备。</p>
      </ul>
      <!--<a target="_blank" class="project-link" href="https://developer.android.google.cn/index.html">Direct to Android</a>-->
    </div>
  </div>
   
   <!-- <h5> 参赛作品要求 </h5>
    <div align="left">  
      <ul> 
        <li>竞赛主题要求</li>
        <p align="left"> 大赛倡导参赛者以促进社会可持续发展作为竞赛主题，关注社区、教育、环保、健康、能源、交通等领域，产生的创意需契合比赛主题，并通过结合创新理念和前沿科技，打造具有社会意义和产业价值的作品。</p>
        <li>竞赛创新性要求</li>
        <p align="left"> 团队可在比赛过程中对作品进行持续的改进，提交的解决方案须具有想象力和创新性；大赛组委会和各分赛区选拔赛承办单位将对其产品进行创新性检索，并将检索结果提交评审委员会作为评分参考。</p>
        <li>作品呈现要求</li>
        <p align="left"> 作品原型要求基于开源软、硬件平台完成；参赛者需要在比赛现场完成设计并制作出可演示的产品原型；作品原型应呈现为实现一定创新功能的智能硬件或软件。</p>
        <li>作品原型要求基于开源软、硬件平台完成</li>
        <p align="left"> 参赛者需要在分赛区预选赛阶段现场完成设计工作，并制作出可演示的产品原型。从分赛区预选赛晋级决赛的团队，需要在决赛阶段现场完成针对产品原型的改进、升级和测试等工作。作品原型应呈现为实现一定创新功能的硬件或软件。</p>
        <li>技术平台要求</li>
        <p align="left"> 组委会将提供大赛可采用的竞赛技术平台的参考方案，参赛者也可自行选择技术平台和使用相应的工具和设备。</p>
      </ul>
    </div> -->
    <!--<p align="left"> A single decision tree is a weak predictor and has its limitations. Gradient boosting is the process of 
      building multiple trees, combining weak predictors to form a strong predicted model. The intuition behind it is that the 
      next iteration corrects the previous error in order to improve overall reliability. The predict result is a weighted 
      combination of each layer of the trees. A simple example is shown below in <i>Figure 3</i>.</p>-->
    <!--<div><img src="{{ "/assets/img/gb_graph.png" | prepend: site.baseurl }}" width="60%" height="60%"/><p>Figure 3: Gradient boosting example</p></div>
 
    <p align="left"> The general steps of gradient boosting is first construct a naive initial tree. Then for each iteration, 
      we construct a new tree to best reduce error using gradient of the loss function. We calculate a weight &Gamma;
      for the prediction result of this tree by minimizing the new loss. The final prediction output would be the weighted 
      sum of results obtained in all iterations.</p>
    
    <div><img src="{{ "/assets/img/gradient_boosting.png" | prepend: site.baseurl }}" width="60%" height="60%"/><p>Figure 4: Gradient boosting pseudocode</p></div>
 
    <p align="left">We identified some challenges with gradient boosting. First, a sequential version itself is hard to implement, 
      requiring deep understanding of machine learning and sophisticated math operations. We decided to use our implementation as baseline. 
      Second, we need to identify potential places to paralize. Building different levels is purely sequential, but there is significant 
      room for parallelization in constructing individual trees. Constructing a decision tree in each iteration requires heavy computation 
      since we need to examine all attributes of all data entries. However, the workload is relatively balanced in this operation. The last 
      concern is the reprentation of data set, as in real world application it is quite large. </p>-->
    <div class="user-details">
    <h5> 知识产权要求 </h5>
    <div align="left">  
      <ul> 
        <li>参赛者必须保证作品的原创性，不得侵犯任何第三方的知识产权或其他权利，且内容符合可适用的法律、法规（包括但不限于中华人民共和国、美利坚合众国的相关法律、法规）。参赛者同意对因侵犯第三方知识产权或其他权利而导致的请求和索赔负全部责任，并保护比赛的主办方、承办方及其代理人并为其辩解，使其不受任何损失赔偿的请求或追诉。</li>
        <li>参赛作品的知识产权归参赛者所有，但应适当兼顾到竞赛主办和承办单位的权益。中华人民共和国教育部作为大赛主办单位，中国（教育部）  留学服务中心、清华大学、北京歌华文化发展集团和谷歌信息技术（中国）有限公司作为大赛承办单位，拥有在全世界范围内永久免费使用本届参赛作品进行演示、部分或全部出版的权利（不涉及技术细节），大赛承办单位的其他全资子公司也拥有上述权利。如果大赛承办单位以其它目的使用参赛作品，需与参赛团队协商，经参赛团队同意后，签署有关对参赛作品使用的协议。</li>
        <li>在可适用的法律允许范围内，大赛工作组保留本规则的最终解释权。</li>
      </ul>
    </div>
    
    <h5> 奖项设置 </h5>
    <p align="left"> 参赛者在比赛中的得分将按照从高到低的顺序排列，以确定以下五个奖项的获奖者（每个团队或个人获得一个“项目奖励”）：</p>
    <div >  
      <ul> 
        <p>一个一等项目奖励（最高得分）</p>
        <p>二个二等项目奖励（排名第二和第三）</p>
        <p>二个三等项目奖励（排名第四和第五）</p>
      </ul>
    </div>
    <p align="center"> 注：如果最终提交的项目作品少于五项，则将不颁发末位项目奖励。</p> 
    <p align="left"> 如果两个或以上项目作品最高得分相同，则裁判通过创新创意维度的评判，全权自主决定从得分相同的参赛作品中选出一个或多个项目，以确定分赛区选拔赛的获奖者。</p>
    
    <!--<h3>比赛报名</h3>
    <h5>报名流程</h5>
    <p align="left">The benchmark result is obtained by running the code on the following platforms:</p>
    <div align="left"> 
      <ul>
        <li>The sequential version is run on machines containing 8 core 3.2 GHz Intel Core i7 processors.</li>
        <li>The Cuda version is run on machines containing <a class="link" target="_blank" href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080/">NVIDIA GeForce GTX 1080</a> . </li>
      </ul>
    </div>  
    
    <h5>Data Set Assumptions</h5>
    
    <p align="left">We obtained training and testing data sets from open source project 
      <a class="link" target="_blank" href="https://github.com/dmlc/xgboost/">xgboost</a>. We slightly modified the original dataset to 
      fit our needs. The largest training dataset contains 1 million entries with 130 attributes. The following are 2 
      requirements on the dataset: </p>
    <div align="left"> 
      <ul>
        <li>All attributes only take binary values, either it’s present or not. The response variable takes 1 and -1.</li>
        <li>And the data set is sparse, meaning that less than half attributes are present for each data entry.</li>
      </ul>
    </div>  
    <p align="left">The reason behind these 2 assumptions relates closely to how we represent input data set. In particular, 
      we use a compressed data structure; instead of parsing all attributes for each data entry, we only keep the attributes 
      that are present. This reduces data communication overhead and scales to larger data sets with memory constraints. More 
      detailed explanation would be provided in the following sections. 
    
    <h3>Approach</h3>
    <h5>Sequential Vesion</h5>
    
    <p align="left">We first construct a sequential version based on the Friedman’s 1999 paper of gradient boosting tree, 
      since our dataset is a binary value attribute dataset and our model is expected to be a binary classifier. 
      This particular model has been discussed in Friedman’s paper. In its paper, Friedman proposed one-step Newton-Raphson 
      approximation of the linear optimization of the loss function. Pseudo-code is provided in <i>Figure5</i>. We directly implemented that approximation since it would 
      take less time than computing the whole gradient of all possible outcome and selecting the best one from it. </p>

    <p align="left">The construction algorithm of decision tree is based on pseudo-code provided by 10-701 course, 
      and we are using entropy approach (ID3 algorithm) to select the best attribute from whole dataset in each iteration. </p>
    
    <div><img src="{{ "/assets/img/gradientboosting.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 5: Gradient boosting with approximation pseudocode</p></div>
 
    <h5>Parallel Attempt 1</h5>
    <p align="left"> In the first attempt of parallelization with CUDA, we simply translate the algorithm directly into 
      CUDA version. We have carefully analyzed the program and we discovered that our approach should be different from 
      previous <a class="link" href="http://zhanpengfang.github.io/418home.html" target="_blank">year project</a> approach. They are using thread parallelization during construction of a decision tree. 
      Such a top-down approach would provide great parallelization on CPU threads with OpenMP. However, since gradient 
      boosting is construct based on weak decision trees that has lower levels, the same top-down approach would waste 
      most of the computing power provided by CUDA and GTX1080. Thus we need to figure out a new data parallel approach. 
      We decided to store Data in a huge GPU array of 130 attributes for each line. Each CUDA thread should read data 
      through bias or indexing of that array. Except the second step in GDBT main loop body, all other three steps are 
      loosely connected. They are suitable to be directly compute through function mapping. The sum of values are done 
      with prefix-sum like reduction kernel. For building decision tree, we use extra two data length int array to represent 
      as mask of values for each decision node. This would avoid the necessity of copying data and distribute new copy 
      as we have done in the sequential version.</p>

     <p align="left"> However, during the implementation, we have realized this approach does not provide good parallelization. 
      The data communication overhead between GBDT steps area high, and each step is mapping their functions to data 
      for only one purpose, which leads to low compute ratio. Furthermore, the way we storing the data consumes lots of 
      memory, but lots of them are only access as an indicator of having or not having an attribute. Therefore, we made 
      several improvements and have a different version of parallelization.</p>

    <h5>Parallel Attempt 2</h5>
    <p align="left">In second version of CUDA parallelization, we have done several things to improve the parallelization 
      of our model. The first thing we do is de-code the algorithm into tiny compute steps to determine potential 
      parallelization in each steps. Thus we figured out with the approximate approach, we are able to compute GBDT 
      loop body step 3, 4 and step 1 of next loop with one reduction and one mapper function. We also figured out during 
      the construction of the tree, several components could be calculating at the same time, since they do not have 
      dependency of each other. Thus we used a single reduction to compute four different attributes to burst the 
      construction of decision tree. </p>
    
    <p align="left">We also changed our data representation, for we have the binary attribute dataset, we could infer 
      the value of attributes by only storing the data that have positive attributes. Thus the data set has been reduced 
      to less than half of the original dataspace. We also stored all intermediate result in the extra few slots of dataline,
      which reduce the communication overhead. Each step only need to access those intermediate result they needed from 
      the mutually agreed position of each dataline. </p>
    
    <p align="left">However, even the improvement significantly speed up our code, we are also introduced new bottleneck 
      into our code. In this version, each thread need to sequentially loop through the entire attribute list to figure out 
      whether a particular attribute is positive or not. Thus we modified our program the third time to deal with newly 
      introduced bottleneck.</p>
    
    <h5>Parallel Attempt 3</h5>
    <p align="left">We observed that in the dataset, all attributes are stored in a ascending order. Thus we would be 
      able to use Binary search to improve the speed up of finding attributes from the current dataline. </p>
    
    <p align="left">After we did this improvement, we analyzed our code again, but we haven’t be able to figure out 
      other point for improving parallelization under current algorithm.</p>
  
    <h3>Results</h3>
    <p align="left">The final result met our expectation. Figure 6 shows how our cuda version code compare to the 
      sequential version code. With increasing of data, cuda shows remarkable parallelization ability and it could 
      achieve about 160x speed up with 1 million dataset. </p>
    
    <p align="left">We have further breakdown the the time of our current cuda model in 1 million dataset to determine 
      the bottleneck of our code. As shown in Figure 7 , no matter how many data is request for copy from host memory 
      to device memory, the CUDA memory copy time of the data does not change much. Most of the time is consumed by 
      building the model, since we have already parallelized the current algorithm, to improve the model building time, 
      it could be only done by adopting faster algorithms.</p>
    
    <p align="left">Distributed GBDT library xgboost does have a CUDA plugin available. However we cannot use it since it 
      cannot compile without cmake. Based on their introductory material, we only know it has ability to execute 1 million 
      records of 50 column in 500 iterations in 193 seconds. Our dataset does not have too much residuals, and it would left 
      with extreme small residuals, which would be considered as zero after 25 iterations. Thus with our approach, and the 
      dataset, we would only need 22 seconds to execute. </p>

    <div><img src="{{ "/assets/img/chart.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 6: Performance analysis and comparison</p></div>
    <div><img src="{{ "/assets/img/breakdown.png" | prepend: site.baseurl }}" width="70%" height="70%"/><p>Figure 7: Parallel implementation time breakdown</p></div>
    
    <h3>Further Research</h3>
    <p align="left">In future research, it is possible to use other algorithms that consumes less time than our 
      current algorithm which is based on Friedman 1999 paper. The paper came up with gradient boosting technique 
      almost 20 years ago, more parallelizable approach would emerge in the passing years, thus GBDT could take less 
      time than our approach. Such as the histogram GBDT algorithm could bring down the execution time even further. </p>
    
    <p align="left">Also, our current implementation is working on single GPU, with expansion of data, the size of  
      dataset will exceed total GPU device memory. Therefore, in order to support larger datasets, CuDB could be extended 
      to support either multiple GPU with a shared device memory or streamly computing of decision tree in single GPU.</p>
    
    <h3>References</h3>
    <p align="left"><cite>Friedman, J. H. (1999). Greedy function approximation: A gradient boosting machine. Retrieved May 12, 2017, 
      from <a class="link" target="_blank" href="http://projecteuclid.org/euclid.aos/1013203451">http://projecteuclid.org/euclid.aos/1013203451</a>.</cite></p>
    <p align="left"><cite>Xing, E. (2015). Decision Tree. <a class="link" target="_blank" href="http://www.cs.cmu.edu/~epxing/Class/10701/slides/DT15New.pdf">Lecture</a>.</cite></p>  
    <p align="left"><cite>Scalable and Flexible Gradient Boosting. (n.d.). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://xgboost.readthedocs.io/en/latest/">https://xgboost.readthedocs.io/en/latest</a>.</cite></p> 
    <p align="left"><cite>Gradient boosting. (2017, April 30). Retrieved May 12, 2017, from <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Gradient_boosting">https://en.wikipedia.org/wiki/Gradient_boosting</a>.</cite></p> --> 
  </div>
</div>
